Regression Model Selection â€” Practical Notes
1ï¸âƒ£ Linear Regression (NO punishment)

Rule

â€œSet knobs wherever you want to minimise error.â€

What happens

Coefficients can become very large

With correlated features â†’ knobs fight each other

Coefficients are unstable and noisy

Implications

Interpretation breaks under multicollinearity

Predictions may still look okay

ğŸ“Œ Use when

Few features

Low correlation

You want a simple baseline

ğŸ“Œ No control over instability

2ï¸âƒ£ Ridge Regression (L2) â€” â€œDonâ€™t let knobs get bigâ€

Rule

â€œYou can use all knobs, but donâ€™t turn any too far.â€

What happens

All coefficients shrink

No coefficient becomes exactly zero

Correlated features share the load

Key idea

Ridge spreads importance across correlated features

So:

weight, displacement, horsepower
â†’ all stay, all shrink


âŒ Ridge does NOT remove features

ğŸ“Œ Use when

Multicollinearity exists

You want stable coefficients

Prediction > feature selection

ğŸ“Œ Ridge = stability, not selection

3ï¸âƒ£ Lasso Regression (L1) â€” â€œSome knobs must be OFFâ€

Rule

â€œYouâ€™re allowed to turn only a few knobs.â€

What happens

Some coefficients become exactly zero

Those features are removed

With correlated features:

Lasso chooses one

Drops the rest arbitrarily

âš ï¸ Important correction
âŒ Lasso does NOT magically find all irrelevant features
âœ… It finds a sparse solution, not a perfect one

ğŸ“Œ Use when

Feature selection is required

High-dimensional data

Correlation is low or moderate

ğŸ“Œ Lasso = feature selection, unstable with correlation

4ï¸âƒ£ Elastic Net (L1 + L2) â€” â€œChoose few knobs, but let them shareâ€

Rule

â€œPick a small set of knobs, but donâ€™t force only one when several are related.â€

What happens

Some coefficients â†’ zero (like Lasso)

Correlated features â†’ kept or dropped together (like Ridge)

Much more stable than Lasso

ğŸ“Œ Use when

Many features

Strong correlation

You want feature selection + stability

ğŸ“Œ Elastic Net = controlled feature selection

5ï¸âƒ£ k-NN Regression â€” â€œLook around, donâ€™t learn a formulaâ€

Rule

â€œPredict using the average of nearby points.â€

What happens

No model is trained

Prediction = average of k nearest neighbors

Non-linear and local

Critical requirements

Features must be scaled

Distance defines everything

Behaviour

Small k â†’ noisy, high variance

Large k â†’ smooth, high bias

Cannot extrapolate beyond data

ğŸ“Œ Use when

Small dataset

Low dimensional data

Strong local patterns

âŒ Do NOT use when

Large datasets

High-dimensional data

Production systems

ğŸ“Œ k-NN = local similarity, not generalisation


Regression Model Selection â€” Practical Notes
1ï¸âƒ£ Linear Regression (NO punishment)

Rule

â€œSet knobs wherever you want to minimise error.â€

What happens

Coefficients can become very large

With correlated features â†’ knobs fight each other

Coefficients are unstable and noisy

Implications

Interpretation breaks under multicollinearity

Predictions may still look okay

ğŸ“Œ Use when

Few features

Low correlation

You want a simple baseline

ğŸ“Œ No control over instability

2ï¸âƒ£ Ridge Regression (L2) â€” â€œDonâ€™t let knobs get bigâ€

Rule

â€œYou can use all knobs, but donâ€™t turn any too far.â€

What happens

All coefficients shrink

No coefficient becomes exactly zero

Correlated features share the load

Key idea

Ridge spreads importance across correlated features

So:

weight, displacement, horsepower
â†’ all stay, all shrink


âŒ Ridge does NOT remove features

ğŸ“Œ Use when

Multicollinearity exists

You want stable coefficients

Prediction > feature selection

ğŸ“Œ Ridge = stability, not selection

3ï¸âƒ£ Lasso Regression (L1) â€” â€œSome knobs must be OFFâ€

Rule

â€œYouâ€™re allowed to turn only a few knobs.â€

What happens

Some coefficients become exactly zero

Those features are removed

With correlated features:

Lasso chooses one

Drops the rest arbitrarily

âš ï¸ Important correction
âŒ Lasso does NOT magically find all irrelevant features
âœ… It finds a sparse solution, not a perfect one

ğŸ“Œ Use when

Feature selection is required

High-dimensional data

Correlation is low or moderate

ğŸ“Œ Lasso = feature selection, unstable with correlation

4ï¸âƒ£ Elastic Net (L1 + L2) â€” â€œChoose few knobs, but let them shareâ€

Rule

â€œPick a small set of knobs, but donâ€™t force only one when several are related.â€

What happens

Some coefficients â†’ zero (like Lasso)

Correlated features â†’ kept or dropped together (like Ridge)

Much more stable than Lasso

ğŸ“Œ Use when

Many features

Strong correlation

You want feature selection + stability

ğŸ“Œ Elastic Net = controlled feature selection

5ï¸âƒ£ k-NN Regression â€” â€œLook around, donâ€™t learn a formulaâ€

Rule

â€œPredict using the average of nearby points.â€

What happens

No model is trained

Prediction = average of k nearest neighbors

Non-linear and local

Critical requirements

Features must be scaled

Distance defines everything

Behaviour

Small k â†’ noisy, high variance

Large k â†’ smooth, high bias

Cannot extrapolate beyond data

ğŸ“Œ Use when

Small dataset

Low dimensional data

Strong local patterns

âŒ Do NOT use when

Large datasets

High-dimensional data

Production systems

ğŸ“Œ k-NN = local similarity, not generalisation


6ï¸âƒ£ Decision Tree Regressor â€” â€œSplit the space, predict the averageâ€
Rule

â€œIf a split reduces error, split again.â€

What happens

The model splits the feature space using ifâ€“else rules

Each split is chosen to reduce variance / MSE

Data is partitioned into regions (leaves)

Prediction = mean of y values in the leaf

How error is measured

Uses variance / MSE reduction

Chooses splits that make child nodes more homogeneous in y

ğŸ“Œ No distance
ğŸ“Œ No coefficients
ğŸ“Œ No scaling required

Behaviour

Captures non-linear relationships

Automatically models feature interactions

Works well with continuous features

âš ï¸ But:

Can keep splitting until it memorises data

Very high variance if unconstrained

Training error â†“, test error â†‘

Why it overfits

Greedy splitting

Can isolate very small groups

Especially bad with:

deep trees

high-cardinality categorical features

How overfitting is controlled

max_depth â†’ limits how deep the tree can grow

min_samples_leaf â†’ enforces minimum samples per prediction

min_samples_split â†’ controls when splitting is allowed

ğŸ“Œ Use when

Data has non-linear patterns

Feature interactions matter

You want rule-based interpretability

As a base model for ensembles (RF, Boosting)

âŒ Do NOT use when

You want best predictive performance

Data is noisy

You donâ€™t restrict tree growth

One-line summary

A decision tree regressor learns ifâ€“else rules that split the feature space to minimise variance, but it overfits unless depth and leaf size are constrained.


7ï¸âƒ£ Random Forest Regressor â€” â€œAverage many overfitted treesâ€
Rule

â€œTrain many different trees on resampled data and average their predictions.â€

What happens

Multiple decision trees are trained

Each tree:

Sees a bootstrap sample of the data (rows sampled with replacement)

Considers only a random subset of features at each split

Each tree predicts:

Mean of y values in its leaf

Final prediction:

Average of predictions from all trees

How error is handled

Each tree minimises variance / MSE locally (greedy splits)

Forest reduces error by:

Averaging uncorrelated tree errors

Variance â†“ significantly

Bias â‰ˆ similar to a single tree

ğŸ“Œ Error reduction comes from averaging, not smarter splits

Key randomness (this is crucial)

Row randomness (Bootstrap)

Sample with replacement

~63% of data seen per tree

~37% is out-of-bag (OOB)

Feature randomness

Only max_features are considered per split

Prevents dominant features from controlling all trees

Prediction mechanism (important)

Tree prediction = mean(y) in leaf

Forest prediction = mean of tree predictions

ğŸ“Œ Mean of means
ğŸ“Œ No distances
ğŸ“Œ No coefficients
ğŸ“Œ No scaling required

Behaviour

Captures complex non-linear relationships

Handles feature interactions automatically

Robust to noise compared to a single tree

Stable predictions (low variance)

âš ï¸ But:

Cannot extrapolate beyond training y range

Slower and more memory-heavy than single trees

Feature importance can be biased

Less interpretable than one tree

Why Random Forest works

Trees are high-variance, low-bias models

Randomisation makes trees less correlated

Averaging cancels out individual tree errors

How overfitting is controlled

n_estimators â†’ more trees â†’ variance â†“

max_depth â†’ limits memorisation

min_samples_leaf â†’ stabilises leaf predictions

max_features â†’ reduces tree correlation

ğŸ“Œ RF overfits much less than a single tree
ğŸ“Œ But deep trees can still overfit if unconstrained

Out-of-Bag (OOB) insight

Each tree leaves out ~37% of data

Those samples can be used for validation

No separate validation set required

ğŸ“Œ Use when

Tabular data

Strong non-linear patterns

Feature interactions matter

You want a strong baseline model

Minimal preprocessing preferred

âŒ Do NOT use when

Data is extremely large (memory constraints)

You need extrapolation

You need simple interpretability

Very high-dimensional sparse data (e.g., text)

One-line summary

A Random Forest Regressor reduces variance by averaging predictions from many decision trees trained on bootstrapped data with feature randomness, making it far more stable than a single tree.

If you want, next I can give you the same note format for:

Gradient Boosting

XGBoost vs RF (interview contrast)

Why RF > linear models on tabular data

Exact biasâ€“variance trade-off explanation

Just say which one.


8ï¸âƒ£ Support Vector Regression (SVR) â€” â€œIgnore small errors, stay flatâ€
Rule

â€œFit the flattest possible function, and ignore errors up to Îµ.â€

What happens

SVR fits a function (line or curve)

It creates an Îµ-tube around the function

Errors inside Îµ are ignored

Errors outside Îµ are penalised

Only some points matter â†’ support vectors

ğŸ“Œ Most points do nothing
ğŸ“Œ Model is defined by a few critical points

How prediction works

Prediction is NOT an average

Prediction is NOT distance-based

Prediction = weighted contribution from support vectors only

Linear SVR:

ğ‘¦
^
=
ğ‘¤
ğ‘¥
+
ğ‘
y
^
	â€‹

=wx+b

Kernel SVR:

ğ‘¦
^
=
âˆ‘
ğ›¼
ğ‘–
ğ¾
(
ğ‘¥
ğ‘–
,
ğ‘¥
)
+
ğ‘
y
^
	â€‹

=âˆ‘Î±
i
	â€‹

K(x
i
	â€‹

,x)+b

ğŸ“Œ Î±áµ¢ â‰  0 only for support vectors
ğŸ“Œ Other points are ignored

How error is measured

Uses Îµ-insensitive loss

Meaning:

|y âˆ’ Å·| â‰¤ Îµ â†’ loss = 0

|y âˆ’ Å·| > Îµ â†’ penalty applied

ğŸ“Œ Focus is on tolerance, not exact fit

Key control knobs

Îµ (epsilon) â†’ how much error you ignore

C â†’ how hard you punish points outside Îµ

kernel â†’ linear vs non-linear shape

ğŸ“Œ Îµ controls noise tolerance
ğŸ“Œ C controls strictness

Behaviour

Produces smooth, stable predictions

Robust to small noise

Works well when relationship is clean and continuous

Uses very few points to define the model

âš ï¸ But:

Very sensitive to feature scaling

Slow for large datasets

Harder to tune than linear models

Less intuitive than trees

ğŸ“Œ Scaling is mandatory

Why SVR works

Minimises model complexity (flatness)

Ignores insignificant errors

Uses only boundary-violating points

Avoids chasing noise

How overfitting is controlled

Îµ â†’ ignores small fluctuations

C â†’ limits how much a point can pull the model

Kernel choice â†’ controls flexibility

ğŸ“Œ Small C + large Îµ â†’ very smooth
ğŸ“Œ Large C + small Îµ â†’ very strict (can overfit)

ğŸ“Œ Use when

Dataset is small to medium

Relationship is smooth

Noise should be ignored

You want controlled generalisation

âŒ Do NOT use when

Dataset is very large

Features are not scaled

Data is highly noisy

You want easy interpretability


7ï¸âƒ£ Random Forest Regressor â€” â€œAverage many noisy treesâ€
Rule

â€œTrain many different trees independently and average their predictions.â€

What happens

Multiple decision trees are trained

Each tree:

Sees a bootstrap sample (rows sampled with replacement)

Uses random subset of features at each split

Each tree overfits slightly

Final prediction = average of all trees

How prediction works

Tree prediction = mean y in leaf

Forest prediction = mean of tree predictions

ğŸ“Œ Mean of means
ğŸ“Œ No coefficients
ğŸ“Œ No distances
ğŸ“Œ No scaling required

Why Random Forest works

Decision trees â†’ high variance

Randomness makes trees less correlated

Averaging cancels individual tree errors

ğŸ“Œ Variance â†“â†“â†“
ğŸ“Œ Bias â‰ˆ same as a single tree

Key randomness (VERY important)

Row randomness (Bootstrap)

~63% data seen per tree

~37% left out â†’ Out-of-Bag (OOB) samples

Feature randomness

Only max_features considered per split

Prevents one strong feature from dominating

Behaviour

Captures non-linear relationships

Automatically models feature interactions

Robust to noise

Very stable predictions

âš ï¸ But:

Cannot extrapolate beyond training target range

Less interpretable than a single tree

Memory & compute heavy

How overfitting is controlled

n_estimators â†’ more trees â†’ variance â†“

max_depth â†’ limits memorisation

min_samples_leaf â†’ stabilises predictions

max_features â†’ reduces tree correlation

ğŸ“Œ RF overfits far less than a single tree
ğŸ“Œ But unconstrained trees can still overfit

ğŸ“Œ Use when

Tabular data

Strong non-linear patterns

Feature interactions matter

Minimal preprocessing preferred

âŒ Do NOT use when

You need extrapolation

Data is extremely large (memory limits)

You need simple interpretability

One-line summary

Random Forest reduces variance by averaging predictions from many decorrelated decision trees trained on bootstrapped data.

8ï¸âƒ£ Gradient Boosting Regressor â€” â€œFix your mistakes step by stepâ€
Rule

â€œTrain trees sequentially, each one correcting the errors of the previous model.â€

What happens

Trees are trained one after another

Each new tree:

Sees the residuals (errors) of the current model

Focuses more on hard-to-predict points

Final prediction = sum of all treesâ€™ outputs

ğŸ“Œ Not averaging
ğŸ“Œ Additive learning
ğŸ“Œ Sequential dependence

How prediction works
Å·(x) = treeâ‚(x) + Î·Â·treeâ‚‚(x) + Î·Â·treeâ‚ƒ(x) + ...


Each tree is a weak learner

Î· = learning rate (shrinkage)

ğŸ“Œ Small corrections added gradually
ğŸ“Œ No distances
ğŸ“Œ No coefficients
ğŸ“Œ No scaling required

How error is handled

Optimises a loss function (default: squared error)

Trees are fit to negative gradients (residuals)

ğŸ“Œ Big errors â†’ more attention
ğŸ“Œ Hard points get more weight

Why Gradient Boosting works

Reduces bias

Converts many weak learners into a strong model

Learns complex, fine-grained patterns

ğŸ“Œ Boosting = learning from mistakes

Behaviour

Extremely powerful on tabular data

Learns complex non-linear relationships

Handles feature interactions well

âš ï¸ But:

Sensitive to outliers

Can overfit noise

Slower than Random Forest

Requires careful tuning

Why boosting is sensitive to outliers

Outliers produce large residuals

Boosting keeps trying to â€œfixâ€ them

Model may chase noise

ğŸ“Œ Especially bad with squared error loss

How overfitting is controlled

learning_rate â†“ â†’ smaller corrections

n_estimators â†‘ â†’ gradual learning

max_depth â†“ â†’ weak learners

subsample < 1 â†’ adds randomness

ğŸ“Œ Small learning rate + shallow trees = best practice

ğŸ“Œ Use when

You want maximum predictive performance

Data has complex structure

You can afford tuning

Dataset is clean or well-regularised

âŒ Do NOT use when

Data has heavy outliers (without robust loss)

You want fast training

You want simple interpretability

One-line summary

Gradient Boosting builds a strong regressor by sequentially adding weak trees that correct previous errors, reducing bias but increasing sensitivity to noise.